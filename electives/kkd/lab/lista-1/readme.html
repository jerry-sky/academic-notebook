<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pl" xml:lang="pl">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jerry Sky" />
  <title>Lista-1</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@500&display=swap" rel="stylesheet">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Lista-1</h1>
<p class="author">
    <span>by </span>
    <span class="author">Jerry Sky</span>
</p>
</header>
<hr />
<h2 id="zadanie-na-laboratorium">Zadanie na laboratorium</h2>
<blockquote>
<p>Dla dyskretnych zmiennych losowych <span class="math inline">\(X\)</span> i <span class="math inline">\(Y\)</span> entropia <span class="math inline">\(Y\)</span> warunkowana przez <span class="math inline">\(X\)</span> jest określona wzorem <span class="math display">\[
H(Y|X) = \sum_{x\in X}P(x) \cdot H(Y|x)
\]</span> gdzie <span class="math display">\[
H(Y|x) = \sum_{y\in Y}P(y|x) \cdot I(y|x)
\]</span> a <span class="math inline">\(P(z)\)</span> oznacza prawdopodobieństwo <span class="math inline">\(z\)</span> a <span class="math inline">\(I(z)\)</span> informację związaną z <span class="math inline">\(z\)</span>.</p>
<p>Napisz program który dla podanego pliku traktowanego jako ciąg 8-bitowych symboli policzy częstość występowania tych symboli oraz częstość występowania symboli po danym symbolu (częstość występowania pod warunkiem, że poprzedni znak jest dany, dla pierwszego znaku przyjmij, że przed nim jest znak o kodzie <span class="math inline">\(0\)</span>). Dopisz funkcje które dla policzonych częstości traktowanych jako zmienne losowe policzy entropię i entropię warunkową (warunkowaną znajomością poprzedniego symbolu), oraz poda różnicę między nimi.</p>
<p>Program ma wypisywać wyniki w sposób czytelny i łatwy do dalszego przetwarzania.</p>
<p>Przeanalizuj wyniki działania swojego programu dla przykładowych plików tekstowych, <code>doc</code>, <code>pdf</code>, <code>mp4</code> czy <code>jpg</code> (weź pliki o rozmiarze co najmniej 1MB).</p>
</blockquote>
<h2 id="uruchomienie-programu">Uruchomienie programu</h2>
<p>W celu uruchomienia programu należy najpierw go skompilować przy pomocy <code>make</code> a następnie <code>./main.out &lt;plik do otwarcia&gt;</code> (przykładowe uruchomienie <code>./main.out pan-tadeusz.txt</code>).</p>
<p>Cały <a href="main.cpp">kod</a> jest zawarty w pliku <code>main.cpp</code>.</p>
<hr />
<p>Poniżej znajdują się dodatkowe uproszczenia wzorów w celu dokładniejszego działania programu.</p>
<h2 id="entropia-warunkowa">Entropia warunkowa</h2>
<p><span class="math inline">\(P(y|x) = \frac{P(y~\cap~x)}{P(x)} = \frac{|y~\cap~x|}{|x|}\)</span></p>
<p><span class="math inline">\(I(y|x) = -\log_2(~P(y|x)~)\)</span></p>
<p><span class="math inline">\(H(Y|x) = \sum_{y \in Y}P(y|x)\cdot I(y|x)\)</span></p>
<p><span class="math inline">\(H(Y|X) = \sum_{x \in X}P(x)\cdot H(Y|x)\)</span></p>
<p>podstawmy:<br />
<span class="math inline">\(H(Y|X) = \sum_{x \in X}(~P(x)\cdot \sum_{y \in Y}P(y|x)\cdot I(y|x)~)\)</span><br />
<span class="math inline">\(H(Y|X) = \sum_{x \in X}(~P(x)\cdot \sum_{y \in Y}P(y|x)\cdot (-\log_2(~P(y|x))~)\)</span></p>
<p>mamy przecież: <span class="math display">\[
P(x) = \frac{|x|}{|\Omega|}
\]</span> oraz: <span class="math display">\[
P(y|x) = \frac{P(y \cap x)}{P(x)} = \frac{|y \cap x|}{|x|}
\]</span> wówczas: <span class="math display">\[
H(Y|X) = \sum_{x \in X}(~\frac{|x|}{|\Omega|}\cdot \sum_{y \in Y}\frac{|y \cap x|}{|x|}\cdot (-\log_2(~\frac{|y \cap x|}{|x|})~)
\]</span> <span class="math display">\[
H(Y|X) = \sum_{x \in X}(~\sum_{y \in Y} \frac{|x|}{|\Omega|} \cdot \frac{|y \cap x|}{|x|}\cdot (-\log_2(~\frac{|y \cap x|}{|x|})~)
\]</span> <span class="math display">\[
H(Y|X) = \sum_{x \in X}(~\sum_{y \in Y} \frac{|y \cap x|}{|\Omega|}\cdot (-\log_2(~\frac{|y \cap x|}{|x|})~)
\]</span> <span class="math display">\[
H(Y|X) = \sum_{x \in X}(~\sum_{y \in Y} \frac{|y \cap x|}{|\Omega|}\cdot (-\log_2(~|y \cap x|)~ + log_2|x|)
\]</span> <span class="math display">\[
H(Y|X) = \sum_{x \in X}(~\sum_{y \in Y} \frac{|y \cap x|}{|\Omega|}\cdot (-\log_2(~|y \cap x|)~ + log_2|x|)
\]</span></p>
<h2 id="zwykła-entropia">Zwykła entropia</h2>
<p><span class="math display">\[
H(X) = \sum_{x \in X}P(x)\cdot I(x)
\]</span> przy czym <span class="math inline">\(I(x) = -\log_2 P(x)\)</span> <span class="math display">\[
H(X) = \sum_{x \in X}\frac{|x|}{|\Omega|}\cdot (-\log_2 \frac{|x|}{|\Omega|})
\]</span> <span class="math display">\[
H(X) = \frac{1}{|\Omega|}\cdot\sum_{x \in X}|x|\cdot (-\log_2 \frac{|x|}{|\Omega|})
\]</span> <span class="math display">\[
H(X) = \frac{1}{|\Omega|}\cdot\sum_{x \in X}|x|\cdot (-\log_2 |x| + log_2 |\Omega|)
\]</span> <span class="math display">\[
H(X) = \frac{1}{|\Omega|}\cdot\sum_{x \in X}|x|\cdot (-\log_2 |x|) + \frac{1}{|\Omega|} \cdot log_2 |\Omega| \cdot \sum_{x \in X} |x|)
\]</span> <span class="math display">\[
H(X) = \frac{1}{|\Omega|}\cdot\sum_{x \in X}|x|\cdot (-\log_2 |x|) + log_2 |\Omega|
\]</span></p>
</body>
</html>
