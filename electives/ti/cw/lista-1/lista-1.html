<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@500&display=swap" rel="stylesheet">
</head>
<body>
<h1 id="lista-1">Lista 1</h1>
<ul>
<li><a href="#zadanie-1">Zadanie 1.</a>
<ul>
<li><a href="#1a">1.a)</a></li>
<li><a href="#1b">1.b)</a></li>
</ul></li>
<li><a href="#zadanie-2">Zadanie 2.</a>
<ul>
<li><a href="#2a">2.a)</a></li>
<li><a href="#2b">2.b)</a></li>
</ul></li>
<li><a href="#zadanie-3">Zadanie 3.</a></li>
<li><a href="#zadanie-4">Zadanie 4.</a></li>
<li><a href="#zadanie-5">Zadanie 5.</a></li>
<li><a href="#zadanie-6">Zadanie 6.</a></li>
<li><a href="#zadanie-7">Zadanie 7.</a></li>
<li><a href="#zadanie-8">Zadanie 8.</a></li>
<li><a href="#zadanie-9">Zadanie 9.</a></li>
</ul>
<hr />
<h2 id="zadanie-1.">Zadanie 1.</h2>
<blockquote>
<p>Oblicz entropię wyniku w następujących eksperymentach</p>
</blockquote>
<h3 id="a">1.a)</h3>
<blockquote>
<p>Rzucamy dwiema kostkami sześciennymi i sumujemy liczbę oczek.</p>
</blockquote>
<p><span class="math inline">\(|\Omega| = 11\)</span></p>
<p><span class="math inline">\(\Omega = \{ 2, 3, \dots, 12 \}\)</span></p>
<p><span class="math display">\[
P(\omega) =
\begin{cases}
    2, 3, 11, 12 &amp; \frac{1}{36}\\
    4, 5, 9, 10 &amp; \frac{2}{36}\\
    6, 7, 8 &amp; \frac{3}{36}\\
\end{cases}
\]</span></p>
<p><span class="math inline">\(\mathrm{H}(\Omega) = \mathrm{E}I\)</span></p>
<p><span class="math inline">\(I(\omega) = -\log_2\left(P(\omega)\right)\)</span></p>
<p><span class="math inline">\(\mathrm{E}I = \sum_{\omega \in \Omega} (P(\omega) \cdot \log_2(\frac{1}{P(\omega)})) = 4 \cdot \frac{1}{36} \cdot \log_2(36) + 4 \cdot \frac{2}{36} \cdot \log_2(18) + 3 \cdot \frac{3}{36} \cdot \log_2(12) = \frac{1}{36}( \log_2(36^4 \cdot 18^4 \cdot 12^3) ) = \dots\)</span></p>
<hr />
<h3 id="b">1.b)</h3>
<blockquote>
<p>Rzucamy dwiema kostkami sześciennymi i mnożymy liczbę oczek.</p>
</blockquote>
<p><span class="math inline">\(\Omega = \{ x\cdot y: x,y \in \{1,\dots,6\}\}\)</span></p>
<p>alternatywnie <span class="math inline">\(\Omega = \{ x\in \{1,\dots, 36\}: x = a\cdot b \land a,b \in \{1,2,\dots,6\} \}\)</span></p>
<div class="figure">
<img src="1.b.multiplication-table.png" />

</div>
<p><span class="math inline">\(|\Omega| = 18\)</span></p>
<p><span class="math display">\[
P(\omega) =
\begin{cases}
    1, 9, 16, 25, 36 &amp; \frac{1}{36}\\
    2, 3, 5, 8, 10, 15, 18, 20, 24, 30 &amp; \frac{2}{36}\\
    4 &amp; \frac{3}{36}\\
    6, 12 &amp; \frac{4}{36}\\
\end{cases}
\]</span></p>
<p><span class="math inline">\(\mathrm{E}I = 5 \cdot \frac{1}{36} \cdot \log 36 + 10 \cdot \frac{2}{36} \cdot \log \frac{36}{2} + 1 \cdot \frac{3}{36} \cdot \frac{36}{3} + 2 \cdot \frac{4}{36} \cdot \log \frac{36}{4} \simeq 4,39\)</span></p>
<hr />
<h2 id="zadanie-2.">Zadanie 2.</h2>
<blockquote>
<p>Oblicz entropię wyniku w następujących eksperymentach</p>
</blockquote>
<h3 id="a-1">2.a)</h3>
<blockquote>
<p>Rzucamy <span class="math inline">\(n\)</span> razy symetryczną monetą i liczymy liczbę orłów.</p>
</blockquote>
<p><span class="math inline">\(|\Omega| = n+1\)</span></p>
<p><span class="math inline">\(\Omega = \{ 0, \dots, n \}\)</span></p>
<p><span class="math display">\[
P(\omega) =
\begin{cases}
    0 &amp; \frac{1}{2^n}\\
    1 &amp; \frac{n}{2^n}\\
    2 &amp; \frac{\binom{n}{2}}{2^n}\\
    \dots\\
    \lfloor \frac{n}{2} \rfloor &amp; \frac{\binom{n}{\lfloor \frac{n}{2} \rfloor}}{2^n}\\
    \dots\\
    n-2 &amp; \frac{\binom{n}{n-2}}{2^n}\\
    n-1 &amp; \frac{n}{2^n}\\
    n &amp; \frac{1}{2^n}
\end{cases}
\]</span></p>
<p><span class="math display">\[
\mathrm{E}I = \sum_{i=0}^{n}(P(\omega) \cdot \log_2(\frac{1}{P(\omega)}) =\\
=
\frac{1}{2^n}\left(\begin{cases}
    \sum_{i=0}^{\frac{n}{2}} \binom{n}{i} \cdot \log_2\left(\frac{2^n}{\binom{n}{i}}\right) &amp; \text{gdy } n \text{ jest parzyste}\\
    \sum_{i=0}^{\frac{n+1}{2}} \binom{n}{i} \cdot \log_2\left(\frac{2^n}{\binom{n}{i}}\right) &amp; \text{gdy } n \text{ jest nieparzyste}
\end{cases}\right) = (*)
\]</span></p>
<p><span class="math display">\[
(*) =
\frac{1}{2^n} \cdot \sum_{i=0}^{k} \binom{n}{i} \cdot \left(n - \log_2\left(\binom{n}{i}\right)\right)
\]</span></p>
<hr />
<h3 id="b-1">2.b)</h3>
<blockquote>
<p>Rzucamy symetryczną monetą do uzyskania pierwszego orła. Wynikiem jest liczba wykonanych rzutów.</p>
</blockquote>
<p><span class="math inline">\(H(X) = \frac{1}{2}\log_2(2) + \frac{1}{4}\log_2(4) + \dots = \sum_{i=1}^{\infty}\frac{1}{2^i}\log_2 2^i = \sum_{i=1}^{\infty}i\frac{1}{2^i} = \sum_{i=1}^{\infty} \frac{1}{2^i} + \sum_{i=2}^{\infty} + \dotsb = \frac{\frac{1}{2}}{1 - \frac{1}{2}} + \frac{\frac{1}{4}}{1 - \frac{1}{2}} + \dotsb = 1 + \frac{1}{2} + \frac{1}{4} + \dots = \sum_{i=0}^{\infty}\frac{1}{2^i} = \frac{1}{1-\frac{1}{2}} = 2\)</span></p>
<hr />
<h2 id="zadanie-3.">Zadanie 3.</h2>
<blockquote>
<p>Niech X będzie dyskretną zmienną losową. Jaka będzie zależność między entropią X a entropią Y jeśli:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y = X + 4\)</span><br />
</li>
<li><span class="math inline">\(Y = 2^X\)</span><br />
</li>
<li><span class="math inline">\(Y = X^2\)</span><br />
</li>
<li><span class="math inline">\(Y = \sin X\)</span></li>
</ol>
</blockquote>
<p>Niech <span class="math inline">\(y = g(x)\)</span>, wówczas: <span class="math display">\[
P(y) = \sum_{x:~ y=g(x)} P(x)
\]</span></p>
<p><span class="math display">\[
\sum_{x:~ y=g(x)}P(x) \log_2P(x) \le \sum_{x:~ y=g(x)}P(x) \log_2 P(y) = P(y) \log_2 P(y)
\]</span></p>
<p><span class="math display">\[
H(X) = -\sum_{x} P(x)\log_2P(x) = -\sum_{y}\sum_{x: y=g(x)} P(x) \log_2 P(x) \ge\\
\ge -\sum_{y} P(y) \log_2 P(y) = H(Y)
\]</span></p>
<h2 id="zadanie-4.">Zadanie 4.</h2>
<blockquote>
<p>Jaka jest maksymalna entropia dyskretnej zmiennej losowej o <span class="math inline">\(N\)</span> wartościach. Podaj wszystkie możliwe rozkłady spełniające to maksimum.</p>
</blockquote>
<p><span class="math inline">\(f(P(x_i) \lambda) \sum_{x_i \in X} \log\frac{1}{P(x_i)} P(x_i) + \lambda(1 - \sum_{x_i \in X}P(x_i))\)</span></p>
<p><span class="math inline">\(1 = \sum P(x_i)\)</span></p>
<p><span class="math inline">\(P(x_i) = \beta_i^2\)</span></p>
<p><span class="math inline">\(\frac{\delta}{\delta} \frac{f(P(x_i), \lambda)}{\beta_i} = 2\beta_i \log\frac{1}{\beta_i^2} - 2\beta_i\frac{1}{\beta_i^2}\beta_i^2 + 2\lambda\beta_i = 0\)</span></p>
<p><span class="math inline">\(2 \log\frac{1}{\beta_i} - 1 + \lambda = 0 \implies \log\frac{1}{\beta_i} = \frac{1 - \lambda}{2} = C\)</span> (<span class="math inline">\(\beta_i = 2^C = C\)</span>)</p>
<p><span class="math inline">\(\frac{\delta}{\delta} \frac{f(P(x_i), \lambda)}{\lambda} = 1 - \sum \beta_i^2 = 0 \implies 1 = N \cdot P(x_i) \implies P(x_i) = \frac{1}{N}\)</span></p>
<hr />
<h2 id="zadanie-5.">Zadanie 5.</h2>
<blockquote>
<p>Jaka jest minimalna entropia dyskretnej zmiennej losowej o <span class="math inline">\(N\)</span> wartościach. Podaj wszystkie możliwe rozkłady spełniające to minimum.</p>
</blockquote>
<p>Hipoteza: Minimalna entropia zachodzi dla rozkładu, gdzie <span class="math inline">\((\exists i)(p_i = 1)\)</span>, gdyż wtedy mamy najwięcej wiedzy wejściowej.</p>
<p>D-d:</p>
<p>Weźmy zatem układ <span class="math inline">\((p_1, \dots, p_N)\)</span>, gdzie <span class="math inline">\(p_i = 1 \land (\forall j \neq i)(p_j = 0)\)</span></p>
<p><span class="math inline">\(H(X) = -\sum_{k=1}^{N} p_k \log_2 p_k = -p_i \log_2 p_i = -\log_2 (1) = 0\)</span></p>
<p>Skoro entropia jest sumą po liczbach postaci <span class="math inline">\(p \log_2 \frac{1}{p}\)</span>, gdzie <span class="math inline">\(p \in [0;1]\)</span> to wiemy, że ów iloczyn nie może przyjąć wartość ujemnej, bo <span class="math inline">\(\log_2 \frac{1}{2} \ge 0\)</span> oraz <span class="math inline">\(p \ge 0\)</span>.</p>
<p>Zatem rozkład pokazany powyżej daje najmniejszą możliwą wartość entropii. Takich rozkładów jest <span class="math inline">\(N\)</span>, gdyż możemy wybrać dowolny <span class="math inline">\(x_i\)</span> jako ten, którego <span class="math inline">\(2(x_i) = 1\)</span>.</p>
<hr />
<p>Jedyny taki rozkład, bo:</p>
<span class="math inline">\(H(X) = -\sum_{i=0}^{N} p_i \underbrace{\log_2 p_i}_{&lt;0} &gt; 0\)</span>, $$
<span class="math display">\[\begin{cases}
    (\exists i) (p_i &gt; 0)\\
    (\forall i) (p_i \neq 1)\\
\end{cases}\]</span>

<p>(j) (p_j (0;1)) $$</p>
<p>Logarytm dla <span class="math inline">\(p_j \in (0;1)\)</span> jest zawsze ujemny zatem suma będzie zawsze dodatnia, bo mnożymy przez <span class="math inline">\(-1\)</span>.</p>
<hr />
<h2 id="zadanie-6.">Zadanie 6.</h2>
<blockquote>
<p>Załóżmy, że mamy dwa źródła <span class="math inline">\(X\)</span> i <span class="math inline">\(Y\)</span> o entropiach <span class="math inline">\(H(X)\)</span> i <span class="math inline">\(H(Y)\)</span>, takie że zbiory ich symboli są rozłączne. Przeprowadzamy losowanie i z prawdopodobieństwem p podajemy symbol ze źródła <span class="math inline">\(X\)</span>, a z prawdopodobieństwem <span class="math inline">\(1 − p\)</span> ze źródła <span class="math inline">\(Y\)</span>. Jaka jest entropia wyniku takiej procedury?</p>
</blockquote>
<p><span class="math inline">\(H(Z) = -\sum_{s \in X} p\cdot q_{x_s} \cdot \log(pq_{x_s} - \sum_{x\in Y}(1-p)q_{y_s} \log((1-p) q_{y_s}) = -p\sum_{s \in X} q_{x_s}(\log p + \log q_{x_s}) - (1-p)\sum_{s \in Y}q_{y_s}(\log (1-p) + \log y_{y_s}) = -p\log p \sum_{s \in X} q_{x_s} - p\sum q_{x_s} \log q_{x_s} - (1-p) \sum q_{y_s} \log q_{y_s} - (1-p) \log(1-p) \sum_{x \in Y} q_{y_s} = -p\log p + p \cdot H(X) + (1-p) \cdot H(Y) - (1-p) \log(1-p) = H(P) + p\cdot H(X) + (1-p) \cdot H(Y)\)</span></p>
<hr />
<h2 id="zadanie-7.">Zadanie 7.</h2>
<blockquote>
<p>Podaj naturalna miarę ilości informacji informacji w przypadku, gdy dysponujemy alfabetem o <span class="math inline">\(a\)</span> znakach. Wypisz jej naturalne własności. Udowodnij, że taka miara jest jedyna.</p>
</blockquote>
<p><span class="math inline">\(f: \mathbb{R}^+ \to \mathbb{R}\)</span> o własnościach: 1. <span class="math inline">\(f(x \cdot y) = f(x) + f(y)\)</span> 2. <span class="math inline">\(\sout{f(1) = 0}\)</span> (niepotrzebny) 3. <span class="math inline">\(f(2) = 1 \qquad\)</span> (może być <span class="math inline">\(f(a) = 1\)</span>)</p>
<p>Jeśli <span class="math inline">\(f\)</span> jest ciągła, wówczas <span class="math inline">\(f(x) = \log_2(x) \qquad (\log_a (x))\)</span>.</p>
<p><strong>D-d</strong></p>
<p>Najpierw patrzymy na pewne oczywiste wartości dyskretne</p>
<ul>
<li><span class="math inline">\(f(1) = f(1 \cdot 1) = f(1) + f(1)\)</span></li>
<li><span class="math inline">\(f(1) = 0\)</span></li>
<li><span class="math inline">\(f(2^n) = f(2 \cdot 2^{n-1}) = f(2) + f(2^{n-1}) = 1 + f(2^{n=1}) = \dots = n\)</span></li>
<li><span class="math inline">\(f(1) = f\left(\frac{1}{2} \cdot 2\right) = f\left(\frac{1}{2}\right) + f(2) = f\left(\frac{1}{2}\right) + 1\)</span></li>
<li><span class="math inline">\(f\left(\frac{1}{2}\right) = 0-1 = -1\)</span></li>
<li>Indukcyjnie <span class="math inline">\(f(2^{-n}) = -n\)</span></li>
</ul>
<p>Czyli narazie mamy:<br />
<img src="7.1.png" /></p>
<p><em>Tylko co z gęstymi kawałkami dziedziny?</em></p>
<p><span class="math inline">\(\frac{p}{2^n},~ p\in \mathbb{N}\)</span></p>
<p><span class="math inline">\(f\left(\frac{p}{q}\right) = f(p) + f\left(\frac{1}{q}\right)\)</span></p>
<hr />
<p><span class="math inline">\(f\left(\frac{1}{q}\right) = -f(q) \qquad f(1) = f\left(q \cdot \frac{1}{q}\right) = \dots\)</span></p>
<hr />
<p><span class="math inline">\(f\left(2^{\frac{p}{q}}\right) = f\left( \left( 2^{\frac{1}{q}} \right) \right) = p\cdot f\left( 2^{\frac{1}{a}} \right) = p\cdot f\)</span></p>
<p><em>do dokończenia — co z liczbami pomiędzy tymi dyskretnymi, które znaleźliśmy?</em></p>
<hr />
<h2 id="zadanie-8.">Zadanie 8.</h2>
<blockquote>
<p>Dla rozkładu <span class="math inline">\(P({x_1}) = \frac{1}{2}, P({x_2}) = \frac{1}{8}, P({x_3}) = P({x_4}) = \dots = P({x_8}) = \frac{1}{16}\)</span> znajdź kody (prefiksowe), tak by średnia długość słowa kodowego była równa entropii.</p>
</blockquote>
<ul>
<li><span class="math inline">\(X = \{x_1, \dots, x_8\}\)</span></li>
<li><span class="math inline">\(H(X) = \frac{1}{2} \log_2 (2) + \frac{1}{8} \log_2(8) + 6 \cdot \frac{1}{16} \log_2 (16) = \frac{1}{2} + \frac{3}{8} + \frac{6}{4} = \frac{19}{8} = 2,375\)</span></li>
</ul>
<p><span class="math inline">\(\mathrm{E}L\)</span> musi wynosić <span class="math inline">\(\frac{19}{8}\)</span></p>
<p><em>(kod prefixowy <span class="math inline">\(\implies\)</span> żadne ze słów kodujących nie jest prefixem innego)</em></p>
<div class="figure">
<img src="8.tabelka.png" />

</div>
<p><span class="math inline">\(\mathrm{E}L = \frac{1}{2} \cdot 1 + \frac{1}{8} \cdot 3 + 6 \cdot 4 \cdot \frac{1}{16} = \frac{8}{16} + \frac{6}{16} + \frac{24}{16} = \frac{38 }{16} = \frac{19}{8}\)</span></p>
<hr />
<h2 id="zadanie-9.">Zadanie 9.</h2>
<blockquote>
<p>Niech <span class="math inline">\(X = \{x_1, x_2, \dots, x_N\}\)</span>. Dla jakich <span class="math inline">\(N\)</span> istnieje rozkład mający kody (prefiksowe) o średniej długości równej entropii?</p>
</blockquote>
<p>Dodajmy dodatkowe założenie, że <span class="math inline">\((\forall i)(P(x = x_i) \in (0;1))\)</span> gdyż inaczej jesteśmy w stanie znaleźć trywialny rozkład z zerami.</p>
<p>Zauważmy, że warunkiem aby entropia była równa średniej długości kodu jest to, że prawdopodobieństwa są potęgami dwójki. Inaczej entropia będzie mniejsza.</p>
<p>Ten warunek możemy sformułować jako dwa pod-warunki: - <span class="math inline">\(a_n \cdot \frac{1}{2^N} + \dotsb a_1 \cdot \frac{1}{2} = 1\)</span> , suma prawdopodobieństw - <span class="math inline">\(\sum_{i=1}^{N} a_i = N\)</span>, mamy <span class="math inline">\(N\)</span> wartości zmiennej losowej</p>
<p>Pytanie czy <span class="math inline">\(\forall N\)</span> znajdziemy takie, <span class="math inline">\(a_n, \dots, a_1\)</span> aby ten warunek zachodził?</p>
<p>Spójrzmy na przykłady z użyciem drzewa binarnego:</p>
<div class="figure">
<img src="9.drzewka.png" />

</div>
<p>Patrząc na te przykłady można wysnuć hipotezę, któ©ą spróbujemy udowodnić: - <span class="math inline">\(h = \lceil \log_2 N \rceil\)</span>, taka wysokość drzewa wystarcza - <span class="math inline">\(a_n = 2N - 2^h \land a_{n-1} = 2^h - N \land (\forall i \neq h,h-1) (a_i = 0)\)</span></p>
<p>Dodając <span class="math inline">\(1\)</span> do <span class="math inline">\(N\)</span> schodzimy jednym wierzchołkiem z poziomu <span class="math inline">\(h-1\)</span> na dwa wierzchołki na poziomie <span class="math inline">\(h\)</span>.</p>
<p><strong>D-d</strong></p>
<ul>
<li><span class="math inline">\(a_N \cdot \frac{1}{2^N} + \dotsb + a_1 \cdot \frac{1}{2} = 1\)</span><br />
<span class="math inline">\(\frac{a_h}{2^h} + \frac{a_{h-1}}{2^{h-1}} = \frac{2N \cdot 2^h}{2^h} + \frac{2^h - N}{2^{h-1}} + \frac{N - 2^{h-1} + 2^h - N}{2^{h-1}} = \frac{2-1}{1} = 1\)</span></li>
<li><span class="math inline">\(\sum_{i=1}^{N} a_i = N\)</span><br />
<span class="math inline">\(\sum_{i=1}^{N} a_i = a_h + a_{h-1} = 2N - 2^h + 2^h - N = N\)</span></li>
</ul>
<hr />
<p>Dla podanego rozkładu oba warunki zachodzą, sprawdźmy zatem czy jego średnia długość kodu równa się entropii.</p>
<ul>
<li><span class="math inline">\(\mathrm{E}\alpha = \overbrace{h}^{\text{długość kodu}} \cdot \overbrace{a_h}^{\text{ilość znaków o tej długości}} \cdot \overbrace{\frac{1}{2^h}}^{\text{prawdopodobieństwo}} + (h-1) a_{h-1} \cdot \frac{1}{2^{h-1}} = \frac{h \cdot a_h}{2^h} + \frac{(h-1)\cdot a_{h-1}}{2^{h-1}}\)</span></li>
<li><span class="math inline">\(H = -\sum_{i=1}^{N} p_i \log_2 p_i = a_h \cdot \left( \frac{1}{2^h} \log_2 2^h \right) + a_{h-1} \left( \frac{1}{2^{h-1}} \log_2 2^{h-1} \right) = \frac{h \cdot a_h}{2^h} + \frac{(h-1) \cdot a_{h-1}}{2^{h-1}} = \mathrm{E}\alpha\)</span></li>
</ul>
<p>Zatem <span class="math inline">\(\forall N\)</span> podany rozkłąd ma średnią długość kodu prefiksowego równą entropii.</p>
<hr />
<p>Przykład użycia <span class="math inline">\(N = 6\)</span> (<span class="math inline">\(h = \lceil \log_2 N \rceil = 3\)</span>)</p>
<ul>
<li><span class="math inline">\(a_2 = 2^3 - 6 =2\)</span></li>
<li><span class="math inline">\(a_3 = 2\cdot 6 - 2^3 = 12 - 8 = 4\)</span></li>
</ul>
<div class="figure">
<img src="9.przykład-N-6-drzewko.png" />

</div>
<p>Kody: - <span class="math inline">\(p_1, p_2 = \frac{1}{4}: \enspace 00,~ 11\)</span> - <span class="math inline">\(p_3,\dots,p_8 = \frac{1}{8}: \enspace 100,~ 101,~ 110,~ 111\)</span></p>
<p>Obliczenia: - <span class="math inline">\(\mathrm{E}\alpha = \frac{4}{4} + \frac{12}{8} = 2.5\)</span> - <span class="math inline">\(H - 2\cdot \frac{1}{4} \cdot \log_2 (4) + 4 \cdot \frac{1}{8} \log_2 (8) = \frac{4}{4} + \frac{12}{8} = 2.5\)</span></p>
<hr />
</body>
</html>
